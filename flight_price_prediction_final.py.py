# -*- coding: utf-8 -*-
"""AerodataCompitition_DataScience.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NFvDpLpU7InyFr2cixoCihJhqp6dPpAS

$$AeroData Competition: Flight Price Prediction$$


---



---


## Objective:
* Building a machine learning model to accurately predict flight ticket prices. This notebook covers the entire process from data exploration and cleaning to feature engineering, model building, and evaluation.
--

---
Developed by: ***SUBHASH KUMAR RANA*** (<a href="https://www.linkedin.com/in/indiansubhashkumar/">Linkedin</a>)/(<a href="https://github.com/subhash2312res664/Indian-Flight-Price-Predictor">Github</a>)

DataSet: (<a href="https://docs.google.com/spreadsheets/d/1hiFFMy4TYevFPocdZONNAbalRS9YwZMg/edit?usp=sharing&ouid=107437412821556658749&rtpof=true&sd=true">Link</a>)

Date: 16th September, 2025.

---
"""

# Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn import metrics
import pickle

# Set plot style
sns.set(style="whitegrid")
plt.style.use("ggplot")

"""# Load and Inspect the Dataset"""

data = pd.read_excel('IndianFlightdata.xlsx')
df = data.copy()

# First 5 rows
print("First 5 rows of the dataset:")
print(df.head())

# Dataset info
print("\nDataset Information:")
df.info()

"""# Data Cleaning and Preprocessing"""

# Check for and handle missing values
print("\nMissing values per column:")
print(df.isnull().sum())
df.dropna(inplace=True)
print("\nMissing values after cleaning:")
print(df.isnull().sum())

"""# --- Feature Extraction from Date/Time Columns ---"""

# Convert 'Date_of_Journey' to datetime
df['Date_of_Journey'] = pd.to_datetime(df['Date_of_Journey'], format='%d/%m/%Y')

# Extract day and month
df['Journey_Day'] = df['Date_of_Journey'].dt.day
df['Journey_Month'] = df['Date_of_Journey'].dt.month

# Extract hour and minute from 'Dep_Time' and 'Arrival_Time'
df['Dep_Hour'] = pd.to_datetime(df['Dep_Time']).dt.hour
df['Dep_Min'] = pd.to_datetime(df['Dep_Time']).dt.minute
df['Arrival_Hour'] = pd.to_datetime(df['Arrival_Time']).dt.hour
df['Arrival_Min'] = pd.to_datetime(df['Arrival_Time']).dt.minute

# Drop original date/time columns
df.drop(['Date_of_Journey', 'Dep_Time', 'Arrival_Time'], axis=1, inplace=True)

# --- Standardizing 'Duration' and 'Total_Stops' ---

# Convert 'Duration' to total minutes
def convert_duration_to_minutes(duration):
    hours = 0
    minutes = 0
    if 'h' in duration:
        hours = int(duration.split('h')[0])
        if 'm' in duration.split('h')[1]:
            minutes = int(duration.split('h')[1].strip().split('m')[0])
    elif 'm' in duration:
        minutes = int(duration.split('m')[0])
    return hours * 60 + minutes

df['Duration_in_Mins'] = df['Duration'].apply(convert_duration_to_minutes)
df.drop('Duration', axis=1, inplace=True)

# Convert 'Total_Stops' to numerical values
stops_mapping = {'non-stop': 0, '1 stop': 1, '2 stops': 2, '3 stops': 3, '4 stops': 4}
df['Total_Stops'] = df['Total_Stops'].map(stops_mapping)

print("\nDataset after preprocessing:")
print(df.head())

"""# Exploratory Data Analysis (EDA)"""

# Price vs. Airline
plt.figure(figsize=(15, 6))
sns.boxplot(x='Airline', y='Price', data=df.sort_values('Price', ascending=False))
plt.title('Price Distribution per Airline')
plt.xticks(rotation=45)
plt.show()

# Price vs. Total Stops
plt.figure(figsize=(8, 5))
sns.barplot(x='Total_Stops', y='Price', data=df)
plt.title('Price vs. Total Number of Stops')
plt.show()

# Price vs. Journey Month
plt.figure(figsize=(8, 5))
sns.lineplot(x='Journey_Month', y='Price', data=df)
plt.title('Price Trend over Months')
plt.ylabel('Average Price')
plt.show()

# Correlation Heatmap
plt.figure(figsize=(12, 12))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""# Handling Categorical Features"""

# Drop 'Route' and 'Additional_Info'
df.drop(['Route', 'Additional_Info'], axis=1, inplace=True)

# One-Hot Encoding for 'Airline', 'Source', 'Destination'
categorical_cols = ['Airline', 'Source', 'Destination']
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded_cols = pd.DataFrame(encoder.fit_transform(df[categorical_cols]))
encoded_cols.columns = encoder.get_feature_names_out(categorical_cols)

# Reset index for concatenation
df.reset_index(drop=True, inplace=True)
encoded_cols.reset_index(drop=True, inplace=True)

# Combine original data with encoded columns
df_final = pd.concat([df.drop(categorical_cols, axis=1), encoded_cols], axis=1)

print("\nFinal dataset shape:", df_final.shape)
print("Final columns:", df_final.columns)

"""# Model Building and Training"""

# Define features (X) and target (y)
X = df_final.drop('Price', axis=1)
y = df_final['Price']

print("\nIdentifying important features using ExtraTreesRegressor...")
selection_model = ExtraTreesRegressor()
selection_model.fit(X, y)

# Plotting feature importances
plt.figure(figsize=(12, 8))
feat_importances = pd.Series(selection_model.feature_importances_, index=X.columns)
feat_importances.nlargest(20).plot(kind='barh')
plt.title("Top 20 Most Important Features")
plt.show()

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the RandomForestRegressor model
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
print("\nTraining the RandomForest Regressor...")
rf_regressor.fit(X_train, y_train)
print("Training complete.")

"""# Model Evaluation"""

# Predictions
y_pred = rf_regressor.predict(X_test)

# Performance Metrics
print("\n--- Initial Model Performance ---")
print('R^2 Score:', metrics.r2_score(y_test, y_pred))
print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, y_pred))
print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

# Plot actual vs. predicted prices
plt.figure(figsize=(10, 6))
sns.regplot(x=y_test, y=y_pred, scatter_kws={'alpha':0.3}, line_kws={'color':'red'})
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Actual vs. Predicted Flight Prices")
plt.show()

"""# Hyperparameter Tuning"""

# Using RandomizedSearchCV to find the best hyperparameters.

print("\nStarting Hyperparameter Tuning...")

# Parameter grid
param_grid = {
    'n_estimators': [100, 200, 300, 500],
    'max_features': ['sqrt', 'log2'],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Randomized Search setup
rf_random_search = RandomizedSearchCV(estimator=rf_regressor, param_distributions=param_grid,
                                      n_iter=50, cv=3, verbose=2, random_state=42, n_jobs=-1)

# Fit the search
rf_random_search.fit(X_train, y_train)

print("\nBest Hyperparameters Found:")
print(rf_random_search.best_params_)

"""# Evaluate the Tuned Model"""

best_rf = rf_random_search.best_estimator_
y_pred_tuned = best_rf.predict(X_test)

print("\n--- Tuned Model Performance ---")
print('R^2 Score:', metrics.r2_score(y_test, y_pred_tuned))
print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, y_pred_tuned))
print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_pred_tuned)))

# Final visualization for the tuned model
plt.figure(figsize=(10, 6))
sns.regplot(x=y_test, y=y_pred_tuned, scatter_kws={'alpha':0.3, 'color':'green'}, line_kws={'color':'blue'})
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price (Tuned)")
plt.title("Actual vs. Predicted Flight Prices (Tuned Model)")
plt.show()

"""# Saving the Final Model"""

print("\nSaving the final model to 'flight_price_model.pkl'")
with open('flight_price_model.pkl', 'wb') as file:
    pickle.dump(best_rf, file)
print("Model saved successfully.")